{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5dfe0fd8-4692-42e2-b56e-84b0f7c1c900",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/19 14:12:31 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"sanity\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5184d145-2f48-41d5-83e3-c08406abc057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=spark.range(5)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d60817d8-9dd6-46a1-95db-8fa4e802fd17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|catalog      |\n",
      "+-------------+\n",
      "|demo         |\n",
      "|spark_catalog|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW CATALOGS\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fbe2cec-c279-4863-b8c0-f841740c6cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.5\n",
      "PySparkShell\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('spark.eventLog.enabled', 'true'),\n",
       " ('spark.driver.extraJavaOptions',\n",
       "  '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'),\n",
       " ('spark.sql.catalogImplementation', 'in-memory'),\n",
       " ('spark.sql.catalog.demo.warehouse', 's3://warehouse/wh/'),\n",
       " ('spark.sql.catalog.demo.io-impl', 'org.apache.iceberg.aws.s3.S3FileIO'),\n",
       " ('spark.app.id', 'local-1758542421044'),\n",
       " ('spark.driver.port', '45215'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.app.name', 'PySparkShell'),\n",
       " ('spark.sql.extensions',\n",
       "  'org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions'),\n",
       " ('spark.history.fs.logDirectory', '/home/iceberg/spark-events'),\n",
       " ('spark.driver.host', '839fa80d5d3b'),\n",
       " ('spark.app.submitTime', '1758542420085'),\n",
       " ('spark.sql.catalog.demo.s3.endpoint', 'http://minio:9000'),\n",
       " ('spark.sql.catalog.demo.uri', 'http://rest:8181'),\n",
       " ('spark.eventLog.dir', '/home/iceberg/spark-events'),\n",
       " ('spark.sql.catalog.demo.type', 'rest'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.executor.extraJavaOptions',\n",
       "  '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'),\n",
       " ('spark.sql.catalog.demo', 'org.apache.iceberg.spark.SparkCatalog'),\n",
       " ('spark.sql.defaultCatalog', 'demo'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.app.startTime', '1758542420264'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(spark.version)\n",
    "print(spark.sparkContext.appName)\n",
    "spark.sparkContext.getConf().getAll()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7936d4c2-333b-4347-9f00-95072b19e594",
   "metadata": {},
   "source": [
    "## Running a small test with sample data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "beec5203-f6b5-4000-9e00-a73e62fcdc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97db248d-1d7a-4c68-a0b1-02e46ad0e8f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/19 14:12:47 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"df-basics\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2c56b59-88a4-4f6d-bc71-963500d6b7e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'df-basics'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.appName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7f11d00-7ccb-4756-9fc2-4976ae0a1e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(\"/home/iceberg/data/testing-spark/orders.csv\")\n",
    "customers = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(\"/home/iceberg/data/testing-spark/customers.csv\")\n",
    "products = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(\"/home/iceberg/data/testing-spark/products.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "924581c7-f28e-42ea-9ecd-195cfc04a679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- order_ts: timestamp (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- qty: integer (nullable = true)\n",
      " |-- order_date: date (nullable = true)\n",
      "\n",
      "root\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- signup_date: date (nullable = true)\n",
      "\n",
      "root\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- sku: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "orders.printSchema(); customers.printSchema(); products.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebe85db0-ce4c-4927-9960-9adcffbda75e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+\n",
      "|customer_id|country|\n",
      "+-----------+-------+\n",
      "|          1|     BG|\n",
      "|          2|     BG|\n",
      "+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "orders = (orders\n",
    "  .withColumn(\"order_date\", F.to_date(\"order_ts\"))\n",
    "        )\n",
    "\n",
    "              \n",
    "bg_customers = customers.filter(F.col(\"country\") == \"BG\").select(\"customer_id\",\"country\")\n",
    "bg_customers.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "24d45c11-6672-4602-b366-3b6c75f4107a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+------+-----+\n",
      "|order_date|gross_sales|orders|units|\n",
      "+----------+-----------+------+-----+\n",
      "|2025-03-21|      47.48|     2|    3|\n",
      "|2025-03-22|      84.99|     3|    5|\n",
      "|2025-03-23|      54.98|     2|    4|\n",
      "|2025-03-24|       41.0|     2|    2|\n",
      "|2025-03-25|      67.47|     3|    4|\n",
      "|2025-03-26|       70.0|     2|    3|\n",
      "|2025-03-27|      67.47|     3|    4|\n",
      "|2025-03-28|       29.0|     1|    1|\n",
      "+----------+-----------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark import StorageLevel\n",
    "\n",
    "line_items = orders\\\n",
    "    .join(products, \"product_id\")\\\n",
    "    .withColumn(\"line_amount\", F.col(\"qty\")*F.col(\"price\"))\\\n",
    "    .persist(StorageLevel.MEMORY_ONLY)  #will bne reused a couple of times. Let's cache it\n",
    "\n",
    "line_items.count()\n",
    "               \n",
    "daily_sales = line_items\\\n",
    "            .groupBy(\"order_date\")\\\n",
    "            .agg(\n",
    "                F.sum(\"line_amount\").alias(\"gross_sales\"),\n",
    "                F.countDistinct(\"order_id\").alias(\"orders\"),\n",
    "                F.sum(\"qty\").alias(\"units\")\n",
    "            )\\\n",
    "            .orderBy(\"order_date\")\n",
    "\n",
    "daily_sales.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e58c5fd0-d9cf-40cb-a3ff-2feffeec42f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "#daily_sales.repartition(1).write.mode(\"overwrite\").partitionBy(\"order_date\").parquet(\"data/testing-spark-output/daily_sales\")\n",
    "\n",
    "daily_sales.repartition(F.col(\"order_date\")).sortWithinPartitions(\"order_date\").write.mode(\"overwrite\").partitionBy(\"order_date\").parquet(\"data/testing-spark-output/daily_sales\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d76f142-1829-4dcd-9517-14a330e0a068",
   "metadata": {},
   "source": [
    "##### Interesting is that first daily_sales is ordered by date but\n",
    "##### this seems to be done only for the benefit of the partitionBy \n",
    "##### when writing to parquet. \n",
    "##### Later on when I read the parquet it reads all the separate partitions but the \"ds\" dataframe is not ordered by date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "229d1d31-bcc3-4d6f-966c-39924728a964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+-----+----------+\n",
      "|gross_sales|orders|units|order_date|\n",
      "+-----------+------+-----+----------+\n",
      "|      47.48|     2|    3|2025-03-21|\n",
      "|      84.99|     3|    5|2025-03-22|\n",
      "|      54.98|     2|    4|2025-03-23|\n",
      "|       41.0|     2|    2|2025-03-24|\n",
      "|      67.47|     3|    4|2025-03-25|\n",
      "|       70.0|     2|    3|2025-03-26|\n",
      "|      67.47|     3|    4|2025-03-27|\n",
      "|       29.0|     1|    1|2025-03-28|\n",
      "+-----------+------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ds = spark.read.parquet(\"data/testing-spark-output/daily_sales\")\n",
    "#ds.show()\n",
    "ds.orderBy(\"order_date\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c5c4993e-2134-4f19-8325-4802ddb92b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Sort ['order_date ASC NULLS FIRST], true\n",
      "+- Aggregate [order_date#86], [order_date#86, sum(line_amount#118) AS gross_sales#140, count(distinct order_id#24) AS orders#141L, sum(qty#28) AS units#143L]\n",
      "   +- Project [product_id#27, order_id#24, order_ts#25, customer_id#26, qty#28, order_date#86, sku#79, category#80, price#81, (cast(qty#28 as double) * price#81) AS line_amount#118]\n",
      "      +- Project [product_id#27, order_id#24, order_ts#25, customer_id#26, qty#28, order_date#86, sku#79, category#80, price#81]\n",
      "         +- Join Inner, (product_id#27 = product_id#78)\n",
      "            :- Project [order_id#24, order_ts#25, customer_id#26, product_id#27, qty#28, to_date(order_ts#25, None, Some(Etc/UTC), false) AS order_date#86]\n",
      "            :  +- Relation [order_id#24,order_ts#25,customer_id#26,product_id#27,qty#28] csv\n",
      "            +- Relation [product_id#78,sku#79,category#80,price#81] csv\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "order_date: date, gross_sales: double, orders: bigint, units: bigint\n",
      "Sort [order_date#86 ASC NULLS FIRST], true\n",
      "+- Aggregate [order_date#86], [order_date#86, sum(line_amount#118) AS gross_sales#140, count(distinct order_id#24) AS orders#141L, sum(qty#28) AS units#143L]\n",
      "   +- Project [product_id#27, order_id#24, order_ts#25, customer_id#26, qty#28, order_date#86, sku#79, category#80, price#81, (cast(qty#28 as double) * price#81) AS line_amount#118]\n",
      "      +- Project [product_id#27, order_id#24, order_ts#25, customer_id#26, qty#28, order_date#86, sku#79, category#80, price#81]\n",
      "         +- Join Inner, (product_id#27 = product_id#78)\n",
      "            :- Project [order_id#24, order_ts#25, customer_id#26, product_id#27, qty#28, to_date(order_ts#25, None, Some(Etc/UTC), false) AS order_date#86]\n",
      "            :  +- Relation [order_id#24,order_ts#25,customer_id#26,product_id#27,qty#28] csv\n",
      "            +- Relation [product_id#78,sku#79,category#80,price#81] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Sort [order_date#86 ASC NULLS FIRST], true\n",
      "+- Aggregate [order_date#86], [order_date#86, sum(line_amount#118) AS gross_sales#140, count(distinct order_id#24) AS orders#141L, sum(qty#28) AS units#143L]\n",
      "   +- Project [order_id#24, qty#28, order_date#86, (cast(qty#28 as double) * price#81) AS line_amount#118]\n",
      "      +- Join Inner, (product_id#27 = product_id#78)\n",
      "         :- Project [order_id#24, product_id#27, qty#28, cast(order_ts#25 as date) AS order_date#86]\n",
      "         :  +- Filter isnotnull(product_id#27)\n",
      "         :     +- Relation [order_id#24,order_ts#25,customer_id#26,product_id#27,qty#28] csv\n",
      "         +- Project [product_id#78, price#81]\n",
      "            +- Filter isnotnull(product_id#78)\n",
      "               +- Relation [product_id#78,sku#79,category#80,price#81] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [order_date#86 ASC NULLS FIRST], true, 0\n",
      "   +- Exchange rangepartitioning(order_date#86 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [plan_id=994]\n",
      "      +- HashAggregate(keys=[order_date#86], functions=[sum(line_amount#118), sum(qty#28), count(distinct order_id#24)], output=[order_date#86, gross_sales#140, orders#141L, units#143L])\n",
      "         +- Exchange hashpartitioning(order_date#86, 200), ENSURE_REQUIREMENTS, [plan_id=991]\n",
      "            +- HashAggregate(keys=[order_date#86], functions=[merge_sum(line_amount#118), merge_sum(qty#28), partial_count(distinct order_id#24)], output=[order_date#86, sum#162, sum#164L, count#167L])\n",
      "               +- HashAggregate(keys=[order_date#86, order_id#24], functions=[merge_sum(line_amount#118), merge_sum(qty#28)], output=[order_date#86, order_id#24, sum#162, sum#164L])\n",
      "                  +- Exchange hashpartitioning(order_date#86, order_id#24, 200), ENSURE_REQUIREMENTS, [plan_id=987]\n",
      "                     +- HashAggregate(keys=[order_date#86, order_id#24], functions=[partial_sum(line_amount#118), partial_sum(qty#28)], output=[order_date#86, order_id#24, sum#162, sum#164L])\n",
      "                        +- Project [order_id#24, qty#28, order_date#86, (cast(qty#28 as double) * price#81) AS line_amount#118]\n",
      "                           +- BroadcastHashJoin [product_id#27], [product_id#78], Inner, BuildRight, false\n",
      "                              :- Project [order_id#24, product_id#27, qty#28, cast(order_ts#25 as date) AS order_date#86]\n",
      "                              :  +- Filter isnotnull(product_id#27)\n",
      "                              :     +- FileScan csv [order_id#24,order_ts#25,product_id#27,qty#28] Batched: false, DataFilters: [isnotnull(product_id#27)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/iceberg/data/testing-spark/orders.csv], PartitionFilters: [], PushedFilters: [IsNotNull(product_id)], ReadSchema: struct<order_id:int,order_ts:timestamp,product_id:int,qty:int>\n",
      "                              +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=982]\n",
      "                                 +- Filter isnotnull(product_id#78)\n",
      "                                    +- FileScan csv [product_id#78,price#81] Batched: false, DataFilters: [isnotnull(product_id#78)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/iceberg/data/testing-spark/products.csv], PartitionFilters: [], PushedFilters: [IsNotNull(product_id)], ReadSchema: struct<product_id:int,price:double>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "daily_sales.explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "41ce9958-aee6-4f63-b27c-f3a13482bcab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "Relation [gross_sales#208,orders#209L,units#210L,order_date#211] parquet\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "gross_sales: double, orders: bigint, units: bigint, order_date: date\n",
      "Relation [gross_sales#208,orders#209L,units#210L,order_date#211] parquet\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Relation [gross_sales#208,orders#209L,units#210L,order_date#211] parquet\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) ColumnarToRow\n",
      "+- FileScan parquet [gross_sales#208,orders#209L,units#210L,order_date#211] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/iceberg/notebooks/notebooks/data/testing-spark-output/daily..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<gross_sales:double,orders:bigint,units:bigint>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ds.explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "454fcf45-3e65-40b5-8b95-d8a5a8f2ddf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StorageLevel(False, False, False, False, 1)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#ds.cache().count()\n",
    "#ds.is_cached\n",
    "ds.storageLevel\n",
    "#ds.unpersist()  # unpersist only this cache\n",
    "#ds.is_cached\n",
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e64c57-9c15-4437-8c72-792d972b3826",
   "metadata": {},
   "source": [
    "### Excercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "35a47ca8-a1ff-4e36-bb57-0184086eab68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-------------------+-----------+---+----------+------------+-----------+-----+-----------+\n",
      "|product_id|order_id|           order_ts|customer_id|qty|order_date|         sku|   category|price|line_amount|\n",
      "+----------+--------+-------------------+-----------+---+----------+------------+-----------+-----+-----------+\n",
      "|        10|    1001|2025-03-21 10:15:00|          1|  2|2025-03-21|S-RED-TSHIRT|    Apparel|19.99|      39.98|\n",
      "|        12|    1002|2025-03-21 11:02:00|          1|  1|2025-03-21|  MUG-COFFEE|       Home|  7.5|        7.5|\n",
      "|        14|    1003|2025-03-22 09:45:00|          2|  1|2025-03-22|  NB-13-CASE|Electronics| 29.0|       29.0|\n",
      "|        13|    1004|2025-03-22 10:10:00|          3|  3|2025-03-22|HDMI-CABL-2M|Electronics| 12.0|       36.0|\n",
      "|        10|    1005|2025-03-22 11:35:00|          5|  1|2025-03-22|S-RED-TSHIRT|    Apparel|19.99|      19.99|\n",
      "|        11|    1006|2025-03-23 08:00:00|          4|  2|2025-03-23|S-BLU-TSHIRT|    Apparel|19.99|      39.98|\n",
      "|        12|    1007|2025-03-23 12:20:00|          3|  2|2025-03-23|  MUG-COFFEE|       Home|  7.5|       15.0|\n",
      "|        14|    1008|2025-03-24 14:50:00|          2|  1|2025-03-24|  NB-13-CASE|Electronics| 29.0|       29.0|\n",
      "|        13|    1009|2025-03-24 15:30:00|          1|  1|2025-03-24|HDMI-CABL-2M|Electronics| 12.0|       12.0|\n",
      "|        10|    1010|2025-03-25 09:15:00|          5|  2|2025-03-25|S-RED-TSHIRT|    Apparel|19.99|      39.98|\n",
      "|        11|    1011|2025-03-25 10:45:00|          4|  1|2025-03-25|S-BLU-TSHIRT|    Apparel|19.99|      19.99|\n",
      "|        12|    1012|2025-03-25 11:30:00|          3|  1|2025-03-25|  MUG-COFFEE|       Home|  7.5|        7.5|\n",
      "|        14|    1013|2025-03-26 13:00:00|          2|  2|2025-03-26|  NB-13-CASE|Electronics| 29.0|       58.0|\n",
      "|        13|    1014|2025-03-26 14:25:00|          1|  1|2025-03-26|HDMI-CABL-2M|Electronics| 12.0|       12.0|\n",
      "|        10|    1015|2025-03-27 10:05:00|          5|  1|2025-03-27|S-RED-TSHIRT|    Apparel|19.99|      19.99|\n",
      "|        11|    1016|2025-03-27 11:50:00|          4|  2|2025-03-27|S-BLU-TSHIRT|    Apparel|19.99|      39.98|\n",
      "|        12|    1017|2025-03-27 12:30:00|          3|  1|2025-03-27|  MUG-COFFEE|       Home|  7.5|        7.5|\n",
      "|        14|    1018|2025-03-28 09:40:00|          2|  1|2025-03-28|  NB-13-CASE|Electronics| 29.0|       29.0|\n",
      "+----------+--------+-------------------+-----------+---+----------+------------+-----------+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "line_items.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4b356c1c-df26-4352-adb3-330aa0fffbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_sales= line_items\\\n",
    "            .groupBy(\"category\")\\\n",
    "            .agg(F.round(F.sum(\"line_amount\"),2).alias(\"revenue\"))\\\n",
    "            .orderBy(F.desc(\"revenue\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e565d57f-6d45-42c0-8ba8-72eab542bcd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [revenue#1325 DESC NULLS LAST], true, 0\n",
      "   +- Exchange rangepartitioning(revenue#1325 DESC NULLS LAST, 200), ENSURE_REQUIREMENTS, [plan_id=1421]\n",
      "      +- HashAggregate(keys=[category#80], functions=[sum(line_amount#423)])\n",
      "         +- Exchange hashpartitioning(category#80, 200), ENSURE_REQUIREMENTS, [plan_id=1418]\n",
      "            +- HashAggregate(keys=[category#80], functions=[partial_sum(line_amount#423)])\n",
      "               +- InMemoryTableScan [category#80, line_amount#423]\n",
      "                     +- InMemoryRelation [product_id#27, order_id#24, order_ts#25, customer_id#26, qty#28, order_date#86, sku#79, category#80, price#81, line_amount#423], StorageLevel(memory, 1 replicas)\n",
      "                           +- AdaptiveSparkPlan isFinalPlan=true\n",
      "                              +- == Final Plan ==\n",
      "                                 *(2) Project [product_id#27, order_id#24, order_ts#25, customer_id#26, qty#28, order_date#86, sku#79, category#80, price#81, (cast(qty#28 as double) * price#81) AS line_amount#423]\n",
      "                                 +- *(2) BroadcastHashJoin [product_id#27], [product_id#78], Inner, BuildRight, false\n",
      "                                    :- *(2) Project [order_id#24, order_ts#25, customer_id#26, product_id#27, qty#28, cast(order_ts#25 as date) AS order_date#86]\n",
      "                                    :  +- *(2) Filter isnotnull(product_id#27)\n",
      "                                    :     +- FileScan csv [order_id#24,order_ts#25,customer_id#26,product_id#27,qty#28] Batched: false, DataFilters: [isnotnull(product_id#27)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/iceberg/data/testing-spark/orders.csv], PartitionFilters: [], PushedFilters: [IsNotNull(product_id)], ReadSchema: struct<order_id:int,order_ts:timestamp,customer_id:int,product_id:int,qty:int>\n",
      "                                    +- BroadcastQueryStage 0\n",
      "                                       +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=1183]\n",
      "                                          +- *(1) Filter isnotnull(product_id#78)\n",
      "                                             +- FileScan csv [product_id#78,sku#79,category#80,price#81] Batched: false, DataFilters: [isnotnull(product_id#78)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/iceberg/data/testing-spark/products.csv], PartitionFilters: [], PushedFilters: [IsNotNull(product_id)], ReadSchema: struct<product_id:int,sku:string,category:string,price:double>\n",
      "                              +- == Initial Plan ==\n",
      "                                 Project [product_id#27, order_id#24, order_ts#25, customer_id#26, qty#28, order_date#86, sku#79, category#80, price#81, (cast(qty#28 as double) * price#81) AS line_amount#423]\n",
      "                                 +- BroadcastHashJoin [product_id#27], [product_id#78], Inner, BuildRight, false\n",
      "                                    :- Project [order_id#24, order_ts#25, customer_id#26, product_id#27, qty#28, cast(order_ts#25 as date) AS order_date#86]\n",
      "                                    :  +- Filter isnotnull(product_id#27)\n",
      "                                    :     +- FileScan csv [order_id#24,order_ts#25,customer_id#26,product_id#27,qty#28] Batched: false, DataFilters: [isnotnull(product_id#27)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/iceberg/data/testing-spark/orders.csv], PartitionFilters: [], PushedFilters: [IsNotNull(product_id)], ReadSchema: struct<order_id:int,order_ts:timestamp,customer_id:int,product_id:int,qty:int>\n",
      "                                    +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=1152]\n",
      "                                       +- Filter isnotnull(product_id#78)\n",
      "                                          +- FileScan csv [product_id#78,sku#79,category#80,price#81] Batched: false, DataFilters: [isnotnull(product_id#78)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/iceberg/data/testing-spark/products.csv], PartitionFilters: [], PushedFilters: [IsNotNull(product_id)], ReadSchema: struct<product_id:int,sku:string,category:string,price:double>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cat_sales.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56efa8c-a830-4ace-b096-773f0ae67a86",
   "metadata": {},
   "source": [
    "#### Note that after caching the polan contains \n",
    " +- AdaptiveSparkPlan isFinalPlan=true\n",
    "                              +- == Final Plan ==\n",
    "                                  \n",
    "which is the orginal plan of the cached bit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6dfa5db5-14d3-4c26-832e-47f20d84061b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|country|orders|\n",
      "+-------+------+\n",
      "|     DE|     3|\n",
      "|     US|     7|\n",
      "|     BG|     8|\n",
      "+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "by_country = orders.join(customers, \"customer_id\")\\\n",
    "  .groupBy(\"country\")\\\n",
    "  .agg(F.countDistinct(\"order_id\").alias(\"orders\"))\n",
    "by_country.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "015258a9-e08e-4e93-b214-dfb69a3c94f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- order_ts: timestamp (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- qty: integer (nullable = true)\n",
      " |-- order_date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "12938297-317f-4d38-9cdb-f75f6f68f0d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----+---+\n",
      "|order_date|product_id|units|rnk|\n",
      "+----------+----------+-----+---+\n",
      "|2025-03-21|        10|    2|  1|\n",
      "|2025-03-22|        13|    3|  1|\n",
      "|2025-03-23|        11|    2|  1|\n",
      "|2025-03-24|        13|    1|  1|\n",
      "|2025-03-25|        10|    2|  1|\n",
      "|2025-03-26|        14|    2|  1|\n",
      "|2025-03-27|        11|    2|  1|\n",
      "|2025-03-28|        14|    1|  1|\n",
      "+----------+----------+-----+---+\n",
      "\n",
      "Spark UI: http://839fa80d5d3b:4041\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "import time\n",
    "w = Window.partitionBy(\"order_date\").orderBy(F.desc(\"units\"))\n",
    "\n",
    "top_per_day = (orders\\\n",
    "  .groupBy(\"order_date\",\"product_id\")\\\n",
    "  .agg(F.sum(\"qty\").alias(\"units\"))\\\n",
    "  .withColumn(\"rnk\", F.row_number().over(w))\\\n",
    "  .filter(\"rnk = 1\"))\n",
    "top_per_day.show()\n",
    "\n",
    "print(\"Spark UI:\", spark.sparkContext.uiWebUrl)\n",
    "time.sleep(120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "22d9f6fb-c2da-43b7-853c-7f547194ae67",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e023f4b-fefd-4847-8970-1602cb1b9c0d",
   "metadata": {},
   "source": [
    "# Day 4\n",
    "## Joins, Aggregations, Spark SQL, Windowing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc161a2f-b713-4f7b-a405-80f59001b910",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions as F\n",
    "spark = SparkSession.builder.appName(\"joins-sql-windows\").getOrCreate()\n",
    "\n",
    "\n",
    "orders = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(\"/home/iceberg/data/testing-spark/orders.csv\") \\\n",
    "    .withColumn(\"order_ts\", F.to_timestamp(\"order_ts\")) \\\n",
    "    .withColumn(\"order_date\", F.to_date(\"order_ts\")) \\\n",
    "    .withColumn(\"qty\", F.col(\"qty\").cast(\"int\"))\n",
    "customers = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(\"/home/iceberg/data/testing-spark/customers.csv\")\n",
    "products = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(\"/home/iceberg/data/testing-spark/products.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae6847d9-df33-4b33-80cf-622e01d86f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- order_ts: timestamp (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- qty: integer (nullable = true)\n",
      " |-- order_date: date (nullable = true)\n",
      "\n",
      "root\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- signup_date: date (nullable = true)\n",
      "\n",
      "root\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- sku: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None, None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders.printSchema(), customers.printSchema(), products.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e58dca4-a729-43c2-9e4d-96a4e9987893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+-------------------+----------+---+----------+----------+---------+-------+-----------+\n",
      "|customer_id|order_id|           order_ts|product_id|qty|order_date|first_name|last_name|country|signup_date|\n",
      "+-----------+--------+-------------------+----------+---+----------+----------+---------+-------+-----------+\n",
      "|          1|    1001|2025-03-21 10:15:00|        10|  2|2025-03-21|      Alex|   Petrov|     BG| 2024-11-01|\n",
      "|          1|    1002|2025-03-21 11:02:00|        12|  1|2025-03-21|      Alex|   Petrov|     BG| 2024-11-01|\n",
      "|          2|    1003|2025-03-22 09:45:00|        14|  1|2025-03-22|     Maria|  Ivanova|     BG| 2024-11-15|\n",
      "|          3|    1004|2025-03-22 10:10:00|        13|  3|2025-03-22|       Sam|       Ng|     US| 2025-01-03|\n",
      "|          5|    1005|2025-03-22 11:35:00|        10|  1|2025-03-22|     Chris|      Lee|     US| 2025-03-11|\n",
      "|          4|    1006|2025-03-23 08:00:00|        11|  2|2025-03-23|      Jana|  Schmidt|     DE| 2025-02-20|\n",
      "|          3|    1007|2025-03-23 12:20:00|        12|  2|2025-03-23|       Sam|       Ng|     US| 2025-01-03|\n",
      "|          2|    1008|2025-03-24 14:50:00|        14|  1|2025-03-24|     Maria|  Ivanova|     BG| 2024-11-15|\n",
      "|          1|    1009|2025-03-24 15:30:00|        13|  1|2025-03-24|      Alex|   Petrov|     BG| 2024-11-01|\n",
      "|          5|    1010|2025-03-25 09:15:00|        10|  2|2025-03-25|     Chris|      Lee|     US| 2025-03-11|\n",
      "|          4|    1011|2025-03-25 10:45:00|        11|  1|2025-03-25|      Jana|  Schmidt|     DE| 2025-02-20|\n",
      "|          3|    1012|2025-03-25 11:30:00|        12|  1|2025-03-25|       Sam|       Ng|     US| 2025-01-03|\n",
      "|          2|    1013|2025-03-26 13:00:00|        14|  2|2025-03-26|     Maria|  Ivanova|     BG| 2024-11-15|\n",
      "|          1|    1014|2025-03-26 14:25:00|        13|  1|2025-03-26|      Alex|   Petrov|     BG| 2024-11-01|\n",
      "|          5|    1015|2025-03-27 10:05:00|        10|  1|2025-03-27|     Chris|      Lee|     US| 2025-03-11|\n",
      "|          4|    1016|2025-03-27 11:50:00|        11|  2|2025-03-27|      Jana|  Schmidt|     DE| 2025-02-20|\n",
      "|          3|    1017|2025-03-27 12:30:00|        12|  1|2025-03-27|       Sam|       Ng|     US| 2025-01-03|\n",
      "|          2|    1018|2025-03-28 09:40:00|        14|  1|2025-03-28|     Maria|  Ivanova|     BG| 2024-11-15|\n",
      "+-----------+--------+-------------------+----------+---+----------+----------+---------+-------+-----------+\n",
      "\n",
      "+-----------+----------+---------+-------+-----------+\n",
      "|customer_id|first_name|last_name|country|signup_date|\n",
      "+-----------+----------+---------+-------+-----------+\n",
      "+-----------+----------+---------+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Join Types \n",
    "\n",
    "inner = orders.join(customers, \"customer_id\")\n",
    "inner.show()\n",
    "no_order_customers = customers.join(orders, \"customer_id\", \"left_anti\")\n",
    "no_order_customers.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "330166e8-0fd2-4eb2-8330-b015ffe21f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+------------------+\n",
      "|order_date|country|           revenue|\n",
      "+----------+-------+------------------+\n",
      "|2025-03-21|     BG|             47.48|\n",
      "|2025-03-22|     US|55.989999999999995|\n",
      "|2025-03-22|     BG|              29.0|\n",
      "|2025-03-23|     DE|             39.98|\n",
      "|2025-03-23|     US|              15.0|\n",
      "|2025-03-24|     BG|              41.0|\n",
      "|2025-03-25|     US|             47.48|\n",
      "|2025-03-25|     DE|             19.99|\n",
      "|2025-03-26|     BG|              70.0|\n",
      "|2025-03-27|     DE|             39.98|\n",
      "|2025-03-27|     US|             27.49|\n",
      "|2025-03-28|     BG|              29.0|\n",
      "+----------+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Spark SQL\n",
    "orders.createOrReplaceTempView(\"orders\")\n",
    "customers.createOrReplaceTempView(\"customers\")\n",
    "products.createOrReplaceTempView(\"products\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT o.order_date, c.country, SUM(o.qty * p.price) AS revenue\n",
    "FROM orders o\n",
    "JOIN customers c USING (customer_id)\n",
    "JOIN products p USING (product_id)\n",
    "GROUP BY o.order_date, c.country\n",
    "ORDER BY o.order_date, revenue DESC\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "819ef3ff-e301-47b5-9c07-c6c4df001d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+---+-----+-----------+\n",
      "|order_id|product_id|qty|price|line_amount|\n",
      "+--------+----------+---+-----+-----------+\n",
      "|    1001|        10|  2|19.99|      39.98|\n",
      "|    1002|        12|  1| 7.50|       7.50|\n",
      "|    1003|        14|  1|29.00|      29.00|\n",
      "|    1004|        13|  3|12.00|      36.00|\n",
      "|    1005|        10|  1|19.99|      19.99|\n",
      "|    1006|        11|  2|19.99|      39.98|\n",
      "|    1007|        12|  2| 7.50|      15.00|\n",
      "|    1008|        14|  1|29.00|      29.00|\n",
      "|    1009|        13|  1|12.00|      12.00|\n",
      "|    1010|        10|  2|19.99|      39.98|\n",
      "|    1011|        11|  1|19.99|      19.99|\n",
      "|    1012|        12|  1| 7.50|       7.50|\n",
      "|    1013|        14|  2|29.00|      58.00|\n",
      "|    1014|        13|  1|12.00|      12.00|\n",
      "|    1015|        10|  1|19.99|      19.99|\n",
      "|    1016|        11|  2|19.99|      39.98|\n",
      "|    1017|        12|  1| 7.50|       7.50|\n",
      "|    1018|        14|  1|29.00|      29.00|\n",
      "+--------+----------+---+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Null handling\n",
    "from pyspark.sql.types import DecimalType\n",
    "\n",
    "li=orders.join(products, \"product_id\")\\\n",
    "        .withColumn(\"price\", F.col(\"price\").cast(DecimalType(10,2)))\\\n",
    "        .withColumn(\"line_amount\", (F.col(\"qty\")*F.col(\"price\")).cast(DecimalType(12,2)))\n",
    "li.select(\"order_id\",\"product_id\",\"qty\",\"price\",\"line_amount\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2e29a8d6-dfef-46b4-92f0-6e4dd75be205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-------+----+\n",
      "|order_date|product_id|revenue|rank|\n",
      "+----------+----------+-------+----+\n",
      "|2025-03-21|        10|  39.98|   1|\n",
      "|2025-03-21|        12|   7.50|   2|\n",
      "|2025-03-22|        13|  36.00|   1|\n",
      "|2025-03-22|        14|  29.00|   2|\n",
      "|2025-03-22|        10|  19.99|   3|\n",
      "|2025-03-23|        11|  39.98|   1|\n",
      "|2025-03-23|        12|  15.00|   2|\n",
      "|2025-03-24|        14|  29.00|   1|\n",
      "|2025-03-24|        13|  12.00|   2|\n",
      "|2025-03-25|        10|  39.98|   1|\n",
      "|2025-03-25|        11|  19.99|   2|\n",
      "|2025-03-25|        12|   7.50|   3|\n",
      "|2025-03-26|        14|  58.00|   1|\n",
      "|2025-03-26|        13|  12.00|   2|\n",
      "|2025-03-27|        11|  39.98|   1|\n",
      "|2025-03-27|        10|  19.99|   2|\n",
      "|2025-03-27|        12|   7.50|   3|\n",
      "|2025-03-28|        14|  29.00|   1|\n",
      "+----------+----------+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Window functions (dense_rank, rolling 2-day)\n",
    "from pyspark.sql.window import Window\n",
    "w_day = Window.partitionBy(\"order_date\").orderBy(F.desc(\"revenue\"))\n",
    "ranked = (li.groupBy(\"order_date\",\"product_id\")\\\n",
    "            .agg(F.sum(\"line_amount\").alias(\"revenue\"))\\\n",
    "            .withColumn(\"rank\", F.dense_rank().over(w_day)))\n",
    "ranked.filter(\"rank <= 1\").orderBy(\"order_date\",\"rank\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d0700e5e-13a3-4cae-b81d-10e02a616784",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/22 13:35:11 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/09/22 13:35:11 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/09/22 13:35:11 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/09/22 13:35:11 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/09/22 13:35:11 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/09/22 13:35:11 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/09/22 13:35:11 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+------+\n",
      "|order_date|  rev|rev_2d|\n",
      "+----------+-----+------+\n",
      "|2025-03-21|47.48| 47.48|\n",
      "|2025-03-22|84.99|132.47|\n",
      "|2025-03-23|54.98|139.97|\n",
      "|2025-03-24|41.00| 95.98|\n",
      "|2025-03-25|67.47|108.47|\n",
      "|2025-03-26|70.00|137.47|\n",
      "|2025-03-27|67.47|137.47|\n",
      "|2025-03-28|29.00| 96.47|\n",
      "+----------+-----+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/22 13:35:11 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/09/22 13:35:11 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "# Rolling 2-day revenue (requires continuous dates to be meaningful; demo only)\n",
    "w_roll = Window.orderBy(\"order_date\").rowsBetween(-1, 0)\n",
    "daily_rev = (li.groupBy(\"order_date\").agg(F.sum(\"line_amount\").alias(\"rev\")))\n",
    "daily_rev.withColumn(\"rev_2d\", F.sum(\"rev\").over(w_roll)).orderBy(\"order_date\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6ec9094b-b94a-4883-88ef-32e463258026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Window in module pyspark.sql.window:\n",
      "\n",
      "class Window(builtins.object)\n",
      " |  Utility functions for defining window in DataFrames.\n",
      " |  \n",
      " |  .. versionadded:: 1.4.0\n",
      " |  \n",
      " |  .. versionchanged:: 3.4.0\n",
      " |      Supports Spark Connect.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  When ordering is not defined, an unbounded window frame (rowFrame,\n",
      " |  unboundedPreceding, unboundedFollowing) is used by default. When ordering is defined,\n",
      " |  a growing window frame (rangeFrame, unboundedPreceding, currentRow) is used by default.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> # ORDER BY date ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW\n",
      " |  >>> window = Window.orderBy(\"date\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
      " |  \n",
      " |  >>> # PARTITION BY country ORDER BY date RANGE BETWEEN 3 PRECEDING AND 3 FOLLOWING\n",
      " |  >>> window = Window.orderBy(\"date\").partitionBy(\"country\").rangeBetween(-3, 3)\n",
      " |  \n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  orderBy(*cols: Union[ForwardRef('ColumnOrName'), List[ForwardRef('ColumnOrName_')]]) -> 'WindowSpec'\n",
      " |      Creates a :class:`WindowSpec` with the ordering defined.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      cols : str, :class:`Column` or list\n",
      " |          names of columns or expressions\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class: `WindowSpec`\n",
      " |          A :class:`WindowSpec` with the ordering defined.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql import Window\n",
      " |      >>> from pyspark.sql.functions import row_number\n",
      " |      >>> df = spark.createDataFrame(\n",
      " |      ...      [(1, \"a\"), (1, \"a\"), (2, \"a\"), (1, \"b\"), (2, \"b\"), (3, \"b\")], [\"id\", \"category\"])\n",
      " |      >>> df.show()\n",
      " |      +---+--------+\n",
      " |      | id|category|\n",
      " |      +---+--------+\n",
      " |      |  1|       a|\n",
      " |      |  1|       a|\n",
      " |      |  2|       a|\n",
      " |      |  1|       b|\n",
      " |      |  2|       b|\n",
      " |      |  3|       b|\n",
      " |      +---+--------+\n",
      " |      \n",
      " |      Show row number order by ``category`` in partition ``id``.\n",
      " |      \n",
      " |      >>> window = Window.partitionBy(\"id\").orderBy(\"category\")\n",
      " |      >>> df.withColumn(\"row_number\", row_number().over(window)).show()\n",
      " |      +---+--------+----------+\n",
      " |      | id|category|row_number|\n",
      " |      +---+--------+----------+\n",
      " |      |  1|       a|         1|\n",
      " |      |  1|       a|         2|\n",
      " |      |  1|       b|         3|\n",
      " |      |  2|       a|         1|\n",
      " |      |  2|       b|         2|\n",
      " |      |  3|       b|         1|\n",
      " |      +---+--------+----------+\n",
      " |  \n",
      " |  partitionBy(*cols: Union[ForwardRef('ColumnOrName'), List[ForwardRef('ColumnOrName_')]]) -> 'WindowSpec'\n",
      " |      Creates a :class:`WindowSpec` with the partitioning defined.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      cols : str, :class:`Column` or list\n",
      " |          names of columns or expressions\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class: `WindowSpec`\n",
      " |          A :class:`WindowSpec` with the partitioning defined.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql import Window\n",
      " |      >>> from pyspark.sql.functions import row_number\n",
      " |      >>> df = spark.createDataFrame(\n",
      " |      ...      [(1, \"a\"), (1, \"a\"), (2, \"a\"), (1, \"b\"), (2, \"b\"), (3, \"b\")], [\"id\", \"category\"])\n",
      " |      >>> df.show()\n",
      " |      +---+--------+\n",
      " |      | id|category|\n",
      " |      +---+--------+\n",
      " |      |  1|       a|\n",
      " |      |  1|       a|\n",
      " |      |  2|       a|\n",
      " |      |  1|       b|\n",
      " |      |  2|       b|\n",
      " |      |  3|       b|\n",
      " |      +---+--------+\n",
      " |      \n",
      " |      Show row number order by ``id`` in partition ``category``.\n",
      " |      \n",
      " |      >>> window = Window.partitionBy(\"category\").orderBy(\"id\")\n",
      " |      >>> df.withColumn(\"row_number\", row_number().over(window)).show()\n",
      " |      +---+--------+----------+\n",
      " |      | id|category|row_number|\n",
      " |      +---+--------+----------+\n",
      " |      |  1|       a|         1|\n",
      " |      |  1|       a|         2|\n",
      " |      |  2|       a|         3|\n",
      " |      |  1|       b|         1|\n",
      " |      |  2|       b|         2|\n",
      " |      |  3|       b|         3|\n",
      " |      +---+--------+----------+\n",
      " |  \n",
      " |  rangeBetween(start: int, end: int) -> 'WindowSpec'\n",
      " |      Creates a :class:`WindowSpec` with the frame boundaries defined,\n",
      " |      from `start` (inclusive) to `end` (inclusive).\n",
      " |      \n",
      " |      Both `start` and `end` are relative from the current row. For example,\n",
      " |      \"0\" means \"current row\", while \"-1\" means one off before the current row,\n",
      " |      and \"5\" means the five off after the current row.\n",
      " |      \n",
      " |      We recommend users use ``Window.unboundedPreceding``, ``Window.unboundedFollowing``,\n",
      " |      and ``Window.currentRow`` to specify special boundary values, rather than using integral\n",
      " |      values directly.\n",
      " |      \n",
      " |      A range-based boundary is based on the actual value of the ORDER BY\n",
      " |      expression(s). An offset is used to alter the value of the ORDER BY expression, for\n",
      " |      instance if the current ORDER BY expression has a value of 10 and the lower bound offset\n",
      " |      is -3, the resulting lower bound for the current row will be 10 - 3 = 7. This however puts a\n",
      " |      number of constraints on the ORDER BY expressions: there can be only one expression and this\n",
      " |      expression must have a numerical data type. An exception can be made when the offset is\n",
      " |      unbounded, because no value modification is needed, in this case multiple and non-numeric\n",
      " |      ORDER BY expression are allowed.\n",
      " |      \n",
      " |      .. versionadded:: 2.1.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      start : int\n",
      " |          boundary start, inclusive.\n",
      " |          The frame is unbounded if this is ``Window.unboundedPreceding``, or\n",
      " |          any value less than or equal to max(-sys.maxsize, -9223372036854775808).\n",
      " |      end : int\n",
      " |          boundary end, inclusive.\n",
      " |          The frame is unbounded if this is ``Window.unboundedFollowing``, or\n",
      " |          any value greater than or equal to min(sys.maxsize, 9223372036854775807).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class: `WindowSpec`\n",
      " |          A :class:`WindowSpec` with the frame boundaries defined,\n",
      " |          from `start` (inclusive) to `end` (inclusive).\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql import Window\n",
      " |      >>> from pyspark.sql import functions as func\n",
      " |      >>> df = spark.createDataFrame(\n",
      " |      ...      [(1, \"a\"), (1, \"a\"), (2, \"a\"), (1, \"b\"), (2, \"b\"), (3, \"b\")], [\"id\", \"category\"])\n",
      " |      >>> df.show()\n",
      " |      +---+--------+\n",
      " |      | id|category|\n",
      " |      +---+--------+\n",
      " |      |  1|       a|\n",
      " |      |  1|       a|\n",
      " |      |  2|       a|\n",
      " |      |  1|       b|\n",
      " |      |  2|       b|\n",
      " |      |  3|       b|\n",
      " |      +---+--------+\n",
      " |      \n",
      " |      Calculate sum of ``id`` in the range from ``id`` of currentRow to ``id`` of currentRow + 1\n",
      " |      in partition ``category``\n",
      " |      \n",
      " |      >>> window = Window.partitionBy(\"category\").orderBy(\"id\").rangeBetween(Window.currentRow, 1)\n",
      " |      >>> df.withColumn(\"sum\", func.sum(\"id\").over(window)).sort(\"id\", \"category\").show()\n",
      " |      +---+--------+---+\n",
      " |      | id|category|sum|\n",
      " |      +---+--------+---+\n",
      " |      |  1|       a|  4|\n",
      " |      |  1|       a|  4|\n",
      " |      |  1|       b|  3|\n",
      " |      |  2|       a|  2|\n",
      " |      |  2|       b|  5|\n",
      " |      |  3|       b|  3|\n",
      " |      +---+--------+---+\n",
      " |  \n",
      " |  rowsBetween(start: int, end: int) -> 'WindowSpec'\n",
      " |      Creates a :class:`WindowSpec` with the frame boundaries defined,\n",
      " |      from `start` (inclusive) to `end` (inclusive).\n",
      " |      \n",
      " |      Both `start` and `end` are relative positions from the current row.\n",
      " |      For example, \"0\" means \"current row\", while \"-1\" means the row before\n",
      " |      the current row, and \"5\" means the fifth row after the current row.\n",
      " |      \n",
      " |      We recommend users use ``Window.unboundedPreceding``, ``Window.unboundedFollowing``,\n",
      " |      and ``Window.currentRow`` to specify special boundary values, rather than using integral\n",
      " |      values directly.\n",
      " |      \n",
      " |      A row based boundary is based on the position of the row within the partition.\n",
      " |      An offset indicates the number of rows above or below the current row, the frame for the\n",
      " |      current row starts or ends. For instance, given a row based sliding frame with a lower bound\n",
      " |      offset of -1 and a upper bound offset of +2. The frame for row with index 5 would range from\n",
      " |      index 4 to index 7.\n",
      " |      \n",
      " |      .. versionadded:: 2.1.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      start : int\n",
      " |          boundary start, inclusive.\n",
      " |          The frame is unbounded if this is ``Window.unboundedPreceding``, or\n",
      " |          any value less than or equal to -9223372036854775808.\n",
      " |      end : int\n",
      " |          boundary end, inclusive.\n",
      " |          The frame is unbounded if this is ``Window.unboundedFollowing``, or\n",
      " |          any value greater than or equal to 9223372036854775807.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class: `WindowSpec`\n",
      " |          A :class:`WindowSpec` with the frame boundaries defined,\n",
      " |          from `start` (inclusive) to `end` (inclusive).\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql import Window\n",
      " |      >>> from pyspark.sql import functions as func\n",
      " |      >>> df = spark.createDataFrame(\n",
      " |      ...      [(1, \"a\"), (1, \"a\"), (2, \"a\"), (1, \"b\"), (2, \"b\"), (3, \"b\")], [\"id\", \"category\"])\n",
      " |      >>> df.show()\n",
      " |      +---+--------+\n",
      " |      | id|category|\n",
      " |      +---+--------+\n",
      " |      |  1|       a|\n",
      " |      |  1|       a|\n",
      " |      |  2|       a|\n",
      " |      |  1|       b|\n",
      " |      |  2|       b|\n",
      " |      |  3|       b|\n",
      " |      +---+--------+\n",
      " |      \n",
      " |      Calculate sum of ``id`` in the range from currentRow to currentRow + 1\n",
      " |      in partition ``category``\n",
      " |      \n",
      " |      >>> window = Window.partitionBy(\"category\").orderBy(\"id\").rowsBetween(Window.currentRow, 1)\n",
      " |      >>> df.withColumn(\"sum\", func.sum(\"id\").over(window)).sort(\"id\", \"category\", \"sum\").show()\n",
      " |      +---+--------+---+\n",
      " |      | id|category|sum|\n",
      " |      +---+--------+---+\n",
      " |      |  1|       a|  2|\n",
      " |      |  1|       a|  3|\n",
      " |      |  1|       b|  3|\n",
      " |      |  2|       a|  2|\n",
      " |      |  2|       b|  5|\n",
      " |      |  3|       b|  3|\n",
      " |      +---+--------+---+\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __annotations__ = {'currentRow': <class 'int'>, 'unboundedFollowing': ...\n",
      " |  \n",
      " |  currentRow = 0\n",
      " |  \n",
      " |  unboundedFollowing = 9223372036854775807\n",
      " |  \n",
      " |  unboundedPreceding = -9223372036854775808\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(pyspark.sql.window.Window)\n",
    "#help(Window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b679ef2d-56fb-4af5-9e64-34661be37170",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "(li.write.mode(\"overwrite\")\n",
    "   .partitionBy(\"order_date\")\n",
    "   .parquet(\"data/testing-spark-output/line_items\"))\n",
    "\n",
    "(ranked.write.mode(\"overwrite\")\n",
    "   .parquet(\"data/testing-spark-output/ranked_products\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d36ad82d-f7c8-4dd8-a208-d7b273e1158f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9e5b4a-102a-4c90-af4d-de6d4f56606d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
