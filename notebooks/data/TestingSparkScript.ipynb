{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5dfe0fd8-4692-42e2-b56e-84b0f7c1c900",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/19 14:12:31 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"sanity\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5184d145-2f48-41d5-83e3-c08406abc057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=spark.range(5)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d60817d8-9dd6-46a1-95db-8fa4e802fd17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|catalog      |\n",
      "+-------------+\n",
      "|demo         |\n",
      "|spark_catalog|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW CATALOGS\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2fbe2cec-c279-4863-b8c0-f841740c6cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.5\n",
      "PySparkShell\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('spark.eventLog.enabled', 'true'),\n",
       " ('spark.app.submitTime', '1758225982978'),\n",
       " ('spark.driver.port', '44679'),\n",
       " ('spark.history.fs.logDirectory', '/home/iceberg/spark-events'),\n",
       " ('spark.sql.warehouse.dir',\n",
       "  'file:/home/iceberg/notebooks/notebooks/spark-warehouse'),\n",
       " ('spark.sql.catalog.demo.s3.endpoint', 'http://minio:9000'),\n",
       " ('spark.eventLog.dir', '/home/iceberg/spark-events'),\n",
       " ('spark.app.startTime', '1758225983187'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.driver.extraJavaOptions',\n",
       "  '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'),\n",
       " ('spark.sql.catalogImplementation', 'in-memory'),\n",
       " ('spark.sql.catalog.demo.warehouse', 's3://warehouse/wh/'),\n",
       " ('spark.sql.catalog.demo.io-impl', 'org.apache.iceberg.aws.s3.S3FileIO'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.app.id', 'local-1758225984083'),\n",
       " ('spark.app.name', 'PySparkShell'),\n",
       " ('spark.sql.extensions',\n",
       "  'org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions'),\n",
       " ('spark.driver.host', '839fa80d5d3b'),\n",
       " ('spark.sql.catalog.demo.uri', 'http://rest:8181'),\n",
       " ('spark.sql.catalog.demo.type', 'rest'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.executor.extraJavaOptions',\n",
       "  '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'),\n",
       " ('spark.sql.catalog.demo', 'org.apache.iceberg.spark.SparkCatalog'),\n",
       " ('spark.sql.defaultCatalog', 'demo'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(spark.version)\n",
    "print(spark.sparkContext.appName)\n",
    "spark.sparkContext.getConf().getAll()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7936d4c2-333b-4347-9f00-95072b19e594",
   "metadata": {},
   "source": [
    "## Running a small test with sample data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "beec5203-f6b5-4000-9e00-a73e62fcdc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97db248d-1d7a-4c68-a0b1-02e46ad0e8f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/19 14:12:47 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"df-basics\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2c56b59-88a4-4f6d-bc71-963500d6b7e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'df-basics'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.appName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7f11d00-7ccb-4756-9fc2-4976ae0a1e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(\"/home/iceberg/data/testing-spark/orders.csv\")\n",
    "customers = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(\"/home/iceberg/data/testing-spark/customers.csv\")\n",
    "products = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(\"/home/iceberg/data/testing-spark/products.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "924581c7-f28e-42ea-9ecd-195cfc04a679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- order_ts: timestamp (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- qty: integer (nullable = true)\n",
      "\n",
      "root\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- signup_date: date (nullable = true)\n",
      "\n",
      "root\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- sku: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "orders.printSchema(); customers.printSchema(); products.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebe85db0-ce4c-4927-9960-9adcffbda75e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+\n",
      "|customer_id|country|\n",
      "+-----------+-------+\n",
      "|          1|     BG|\n",
      "|          2|     BG|\n",
      "+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "orders = (orders\n",
    "  .withColumn(\"order_date\", F.to_date(\"order_ts\"))\n",
    "        )\n",
    "\n",
    "              \n",
    "bg_customers = customers.filter(F.col(\"country\") == \"BG\").select(\"customer_id\",\"country\")\n",
    "bg_customers.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24d45c11-6672-4602-b366-3b6c75f4107a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+------+-----+\n",
      "|order_date|gross_sales|orders|units|\n",
      "+----------+-----------+------+-----+\n",
      "|2025-03-21|      47.48|     2|    3|\n",
      "|2025-03-22|      84.99|     3|    5|\n",
      "|2025-03-23|      54.98|     2|    4|\n",
      "|2025-03-24|       41.0|     2|    2|\n",
      "|2025-03-25|      67.47|     3|    4|\n",
      "|2025-03-26|       70.0|     2|    3|\n",
      "|2025-03-27|      67.47|     3|    4|\n",
      "|2025-03-28|       29.0|     1|    1|\n",
      "+----------+-----------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "line_items = orders\\\n",
    "    .join(products, \"product_id\")\\\n",
    "    .withColumn(\"line_amount\", F.col(\"qty\")*F.col(\"price\"))\n",
    "               \n",
    "daily_sales = line_items\\\n",
    "            .groupBy(\"order_date\")\\\n",
    "            .agg(\n",
    "                F.sum(\"line_amount\").alias(\"gross_sales\"),\n",
    "                F.countDistinct(\"order_id\").alias(\"orders\"),\n",
    "                F.sum(\"qty\").alias(\"units\")\n",
    "            )\\\n",
    "            .orderBy(\"order_date\")\n",
    "\n",
    "daily_sales.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e58c5fd0-d9cf-40cb-a3ff-2feffeec42f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "daily_sales.repartition(1).write.mode(\"overwrite\").partitionBy(\"order_date\").parquet(\"data/testing-spark-output/daily_sales\")\n",
    "\n",
    "#daily_sales.repartition(F.col(\"order_date\")).sortWithinPartitions(\"order_date\").mode(\"overwrite\").partitionBy(\"order_date\").parquet(\"data/testing-spark-output/daily_sales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "229d1d31-bcc3-4d6f-966c-39924728a964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+-----+----------+\n",
      "|gross_sales|orders|units|order_date|\n",
      "+-----------+------+-----+----------+\n",
      "|      47.48|     2|    3|2025-03-21|\n",
      "|      84.99|     3|    5|2025-03-22|\n",
      "|      54.98|     2|    4|2025-03-23|\n",
      "|       41.0|     2|    2|2025-03-24|\n",
      "|      67.47|     3|    4|2025-03-25|\n",
      "|       70.0|     2|    3|2025-03-26|\n",
      "|      67.47|     3|    4|2025-03-27|\n",
      "|       29.0|     1|    1|2025-03-28|\n",
      "+-----------+------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ds = spark.read.parquet(\"data/testing-spark-output/daily_sales\")\n",
    "#ds.show()\n",
    "ds.orderBy(\"order_date\").show()\n",
    "\n",
    "#Interesting is that first daily_sales is ordered by date but\n",
    "#this seems to be done only for the benefit of the partitionBy \n",
    "#when writing to parquet. \n",
    "#Later on when I read the parquet it reads all the separate partitions but the \"ds\" dataframe is not ordered by date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c4993e-2134-4f19-8325-4802ddb92b97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
